{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edc9ca08",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 44 is out of bounds for axis 1 with size 44",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 109\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Create dummy data with adjusted features\u001b[39;00m\n\u001b[1;32m    107\u001b[0m base_folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/gabrielalvesiervolino/Desktop/Coding/machineLearning/voice_recog_final_project/audio_processing/data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 109\u001b[0m X, y \u001b[38;5;241m=\u001b[39m extractData(base_folder_path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/barbie/barbie_0.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# Normalize data\u001b[39;00m\n\u001b[1;32m    112\u001b[0m X \u001b[38;5;241m=\u001b[39m normalize(X)\n",
      "Cell \u001b[0;32mIn[3], line 101\u001b[0m, in \u001b[0;36mextractData\u001b[0;34m(path, n_fft, hop_length)\u001b[0m\n\u001b[1;32m     99\u001b[0m stft_db \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mamplitude_to_db(\u001b[38;5;28mabs\u001b[39m(stft), ref\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mmax)\n\u001b[1;32m    100\u001b[0m shape \u001b[38;5;241m=\u001b[39m stft_db\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stft_db[:, \u001b[38;5;241m0\u001b[39m:shape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], stft_db[:, shape[\u001b[38;5;241m1\u001b[39m]]\n",
      "\u001b[0;31mIndexError\u001b[0m: index 44 is out of bounds for axis 1 with size 44"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from scipy.signal import butter, filtfilt\n",
    "import audiosegment\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Function to create dummy data\n",
    "def create_data(points, classes): \n",
    "    x = np.zeros((points * classes, 64))  # Adjusted to 64 features\n",
    "    y = np.zeros(points * classes, dtype='uint8')\n",
    "    for class_number in range(classes):\n",
    "        ix = range(points * class_number, points * (class_number + 1))\n",
    "        r = np.linspace(0.0, 1, points)  # radius \n",
    "        t = np.linspace(class_number * 4, (class_number + 1) * 4, points) + np.random.randn(points) * 0.2\n",
    "        features = np.c_[r * np.sin(t * 2.5), r * np.cos(t * 2.5)]  # Generate 2 features\n",
    "        x[ix, :2] = features  # Place the 2 features in the first 2 columns\n",
    "        y[ix] = class_number\n",
    "    return x, y\n",
    "\n",
    "# Normalize data with a small constant to avoid division by zero\n",
    "def normalize(X, epsilon=1e-8):\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std_dev = np.std(X, axis=0)\n",
    "    std_dev[std_dev == 0] = epsilon  # Set std_dev to epsilon where it's zero\n",
    "    return (X - mean) / std_dev\n",
    "\n",
    "# Layers and activations\n",
    "class Layer:\n",
    "    def __init__(self, input_size, neuron_size):\n",
    "        self.weights = np.random.randn(input_size, neuron_size) * np.sqrt(2.0 / input_size)\n",
    "        self.biases = np.zeros((1, neuron_size))\n",
    "        self.dw = np.zeros_like(self.weights)\n",
    "        self.db = np.zeros_like(self.biases)\n",
    "        self.v_dw = np.zeros_like(self.weights)\n",
    "        self.v_db = np.zeros_like(self.biases)\n",
    "        self.s_dw = np.zeros_like(self.weights)\n",
    "        self.s_db = np.zeros_like(self.biases)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, inputs, grad_output, learning_rate):\n",
    "        grad_inputs = np.dot(grad_output, self.weights.T)\n",
    "        grad_weights = np.dot(inputs.T, grad_output)\n",
    "        grad_biases = np.sum(grad_output, axis=0, keepdims=True)\n",
    "        \n",
    "        # Adam optimizer\n",
    "        beta1 = 0.9\n",
    "        beta2 = 0.999\n",
    "        epsilon = 1e-7\n",
    "\n",
    "        self.v_dw = beta1 * self.v_dw + (1 - beta1) * grad_weights\n",
    "        self.v_db = beta1 * self.v_db + (1 - beta1) * grad_biases\n",
    "        self.s_dw = beta2 * self.s_dw + (1 - beta2) * (grad_weights ** 2)\n",
    "        self.s_db = beta2 * self.s_db + (1 - beta2) * (grad_biases ** 2)\n",
    "        \n",
    "        v_dw_corrected = self.v_dw / (1 - beta1)\n",
    "        v_db_corrected = self.v_db / (1 - beta1)\n",
    "        s_dw_corrected = self.s_dw / (1 - beta2)\n",
    "        s_db_corrected = self.s_db / (1 - beta2)\n",
    "        \n",
    "        self.weights -= learning_rate * v_dw_corrected / (np.sqrt(s_dw_corrected) + epsilon)\n",
    "        self.biases -= learning_rate * v_db_corrected / (np.sqrt(s_db_corrected) + epsilon)\n",
    "\n",
    "class ReluActivation:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "class SoftmaxActivation:\n",
    "    def forward(self, inputs):\n",
    "        normalized_inputs = np.exp(inputs - np.amax(inputs, axis=1, keepdims=True))\n",
    "        self.output = normalized_inputs / np.sum(normalized_inputs, axis=1, keepdims=True)\n",
    "\n",
    "class CrossEntropyLoss:\n",
    "    @staticmethod\n",
    "    def calculate_loss(y_pred, y_true):\n",
    "        clipped_values = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = clipped_values[range(len(clipped_values)), y_true]\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(clipped_values * y_true, axis=1)\n",
    "        return np.mean(-np.log(correct_confidences))\n",
    "\n",
    "def calculate_accuracy(y_pred, y_true):\n",
    "    predicted_classes = np.argmax(y_pred, axis=1)\n",
    "    correct_predictions = np.sum(predicted_classes == y_true)\n",
    "    accuracy = correct_predictions / len(y_true)\n",
    "    return accuracy\n",
    "\n",
    "def extractData(path, n_fft=2048, hop_length=512):\n",
    "    y, sr = librosa.load(path)\n",
    "    normalized = librosa.util.normalize(y)\n",
    "\n",
    "    stft = librosa.core.stft(normalized, n_fft=n_fft, hop_length=hop_length)\n",
    "    stft_db = librosa.amplitude_to_db(abs(stft), ref=np.max)\n",
    "    shape = stft_db.shape\n",
    "    return stft_db[:, 0:shape[1]-2], stft_db[:, shape[1]-1]\n",
    "\n",
    "\n",
    "\n",
    "# Create dummy data with adjusted features\n",
    "\n",
    "base_folder_path = \"/Users/gabrielalvesiervolino/Desktop/Coding/machineLearning/voice_recog_final_project/audio_processing/data\"\n",
    "\n",
    "X, y = extractData(base_folder_path+\"/barbie/barbie_0.wav\")\n",
    "\n",
    "# Normalize data\n",
    "X = normalize(X)\n",
    "\n",
    "# Initialize layers and activations\n",
    "l1 = Layer(64, 128)  # Increased neurons in the first layer for better representation\n",
    "ac1 = ReluActivation()\n",
    "l2 = Layer(128, 64)  # Added an additional hidden layer\n",
    "ac2 = ReluActivation()\n",
    "l3 = Layer(64, 3)    # Output layer should match the number of classes (3)\n",
    "ac3 = SoftmaxActivation()\n",
    "\n",
    "# Training loop parameters\n",
    "epochs = 11000  # Reduced number of epochs to avoid overfitting\n",
    "learning_rate = 0.01  # Adjusted learning rate\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    l1.forward(X)\n",
    "    ac1.forward(l1.output)\n",
    "    l2.forward(ac1.output)\n",
    "    ac2.forward(l2.output)\n",
    "    l3.forward(ac2.output)\n",
    "    ac3.forward(l3.output)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = CrossEntropyLoss.calculate_loss(ac3.output, y)\n",
    "\n",
    "    # Backward pass (Gradient calculation)\n",
    "    grad_ac3 = ac3.output.copy()\n",
    "    grad_ac3[range(len(y)), y] -= 1\n",
    "    grad_ac3 /= len(y)\n",
    "\n",
    "    # Backpropagate gradients\n",
    "    ac3_backward = grad_ac3\n",
    "    l3.backward(ac2.output, ac3_backward, learning_rate)\n",
    "\n",
    "    ac2_backward = np.dot(ac3_backward, l3.weights.T)\n",
    "    l2.backward(ac1.output, ac2_backward, learning_rate)\n",
    "\n",
    "    ac1_backward = np.dot(ac2_backward, l2.weights.T)\n",
    "    l1.backward(X, ac1_backward, learning_rate)\n",
    "\n",
    "    # Print loss and accuracy every 500 epochs\n",
    "    if epoch % 500 == 0:\n",
    "        accuracy = calculate_accuracy(ac3.output, y)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Final loss and accuracy\n",
    "l1.forward(X)\n",
    "ac1.forward(l1.output)\n",
    "l2.forward(ac1.output)\n",
    "ac2.forward(l2.output)\n",
    "l3.forward(ac2.output)\n",
    "ac3.forward(l3.output)\n",
    "final_loss = CrossEntropyLoss.calculate_loss(ac3.output, y)\n",
    "final_accuracy = calculate_accuracy(ac3.output, y)\n",
    "print(np.argmax(ac3.output, axis=1), \"sss\", y)\n",
    "print(\"Final Loss:\", final_loss)\n",
    "print(\"Final Accuracy:\", final_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84088987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from torch.nn import Module\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import Linear\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import LogSoftmax\n",
    "from torch import flatten\n",
    "\n",
    "\n",
    "class LeNet(Module):\n",
    "    def __init__(self, numChannels, classes) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # initialize first set of CONV => RELU => POOL layers\n",
    "        self.conv1 = Conv2d(in_channels=numChannels, out_channels=20, kernel_size=(5, 5))\n",
    "        self.relu = ReLU()\n",
    "        self.maxpool1 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "\n",
    "        # initialize second set of CONV => RELU => POOL layers\n",
    "        self.conv2 = Conv2d(in_channels=20, out_channels=50, kernel_size=(5, 5))\n",
    "        self.relu2 = ReLU()\n",
    "        self.maxpool2 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "\n",
    "\t\t# initialize first (and only) set of FC => RELU layers\n",
    "        self.fc1 = Linear(in_features=800, out_features=500)\n",
    "        self.relu3 = ReLU()\n",
    "\n",
    "        # initialize our softmax classifier\n",
    "        self.fc2 = Linear(in_features=500, out_features=classes)\n",
    "        self.logSoftmax = LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # pass the input through our first set of CONV => RELU =>\n",
    "\t\t# POOL layers    \n",
    "        X = self.conv1(X)\n",
    "        X = self.relu(X)\n",
    "        X = self.maxpool1(X)\n",
    "\n",
    "\t\t# pass the output from the previous layer through the second\n",
    "\t\t# set of CONV => RELU => POOL layers\n",
    "        X = self.conv2(X)\n",
    "        X = self.relu2(X)\n",
    "        X = self.maxpool2(X)\n",
    "\n",
    "\n",
    "\t\t# flatten the output from the previous layer and pass it\n",
    "\t\t# through our only set of FC => RELU layers\n",
    "        X = flatten(X, 1)\n",
    "        X = self.fc1(X)\n",
    "        X = self.relu3(X)\n",
    "\n",
    "        # pass the output to our softmax classifier to get our output\n",
    "        # predictions\n",
    "        x = self.fc2(x)\n",
    "        output = self.logSoftmax(x)\n",
    "\n",
    "        # return the output predictions\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "114e4301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the matplotlib backend so figures can be saved in the background\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "# import the necessary packages\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets import KMNIST\n",
    "from torch.optim import Adam\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# define training hyperparameters\n",
    "INIT_LR = 1e-3\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "# define the train and val splits\n",
    "TRAIN_SPLIT = 0.75\n",
    "VAL_SPLIT = 1 - TRAIN_SPLIT\n",
    "\n",
    "# set the device we will be using to train the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# initialize the LeNet model\n",
    "print(\"[INFO] initializing the LeNet model...\")\n",
    "model = LeNet(\n",
    "\tnumChannels=1,\n",
    "\tclasses=len(trainData.dataset.classes)).to(device)\n",
    "# initialize our optimizer and loss function\n",
    "opt = Adam(model.parameters(), lr=INIT_LR)\n",
    "lossFn = nn.NLLLoss()\n",
    "# initialize a dictionary to store training history\n",
    "H = {\n",
    "\t\"train_loss\": [],\n",
    "\t\"train_acc\": [],\n",
    "\t\"val_loss\": [],\n",
    "\t\"val_acc\": []\n",
    "}\n",
    "# measure how long training is going to take\n",
    "print(\"[INFO] training the network...\")\n",
    "startTime = time.time()\n",
    "\n",
    "\n",
    "# loop over our epochs\n",
    "for e in range(0, EPOCHS):\n",
    "\t# set the model in training mode\n",
    "\tmodel.train()\n",
    "\t# initialize the total training and validation loss\n",
    "\ttotalTrainLoss = 0\n",
    "\ttotalValLoss = 0\n",
    "\t# initialize the number of correct predictions in the training\n",
    "\t# and validation step\n",
    "\ttrainCorrect = 0\n",
    "\tvalCorrect = 0\n",
    "\t# loop over the training set\n",
    "\tfor (x, y) in trainDataLoader:\n",
    "\t\t# send the input to the device\n",
    "\t\t(x, y) = (x.to(device), y.to(device))\n",
    "\t\t# perform a forward pass and calculate the training loss\n",
    "\t\tpred = model(x)\n",
    "\t\tloss = lossFn(pred, y)\n",
    "\t\t# zero out the gradients, perform the backpropagation step,\n",
    "\t\t# and update the weights\n",
    "\t\topt.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\topt.step()\n",
    "\t\t# add the loss to the total training loss so far and\n",
    "\t\t# calculate the number of correct predictions\n",
    "\t\ttotalTrainLoss += loss\n",
    "\t\ttrainCorrect += (pred.argmax(1) == y).type(\n",
    "\t\t\ttorch.float).sum().item()\n",
    "\t\t\n",
    "\n",
    "\n",
    "# switch off autograd for evaluation\n",
    "\twith torch.no_grad():\n",
    "\t\t# set the model in evaluation mode\n",
    "\t\tmodel.eval()\n",
    "\t\t# loop over the validation set\n",
    "\t\tfor (x, y) in valDataLoader:\n",
    "\t\t\t# send the input to the device\n",
    "\t\t\t(x, y) = (x.to(device), y.to(device))\n",
    "\t\t\t# make the predictions and calculate the validation loss\n",
    "\t\t\tpred = model(x)\n",
    "\t\t\ttotalValLoss += lossFn(pred, y)\n",
    "\t\t\t# calculate the number of correct predictions\n",
    "\t\t\tvalCorrect += (pred.argmax(1) == y).type(\n",
    "\t\t\t\ttorch.float).sum().item()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
