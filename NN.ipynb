{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc9ca08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Function to create dummy data\n",
    "def create_data(points, classes): \n",
    "    x = np.zeros((points * classes, 64))  # Adjusted to 64 features\n",
    "    y = np.zeros(points * classes, dtype='uint8')\n",
    "    for class_number in range(classes):\n",
    "        ix = range(points * class_number, points * (class_number + 1))\n",
    "        r = np.linspace(0.0, 1, points)  # radius \n",
    "        t = np.linspace(class_number * 4, (class_number + 1) * 4, points) + np.random.randn(points) * 0.2\n",
    "        features = np.c_[r * np.sin(t * 2.5), r * np.cos(t * 2.5)]  # Generate 2 features\n",
    "        x[ix, :2] = features  # Place the 2 features in the first 2 columns\n",
    "        y[ix] = class_number\n",
    "    return x, y\n",
    "\n",
    "# Normalize data with a small constant to avoid division by zero\n",
    "def normalize(X, epsilon=1e-8):\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std_dev = np.std(X, axis=0)\n",
    "    std_dev[std_dev == 0] = epsilon  # Set std_dev to epsilon where it's zero\n",
    "    return (X - mean) / std_dev\n",
    "\n",
    "# Layers and activations\n",
    "class Layer:\n",
    "    def __init__(self, input_size, neuron_size):\n",
    "        self.weights = np.random.randn(input_size, neuron_size) * np.sqrt(2.0 / input_size)\n",
    "        self.biases = np.zeros((1, neuron_size))\n",
    "        self.dw = np.zeros_like(self.weights)\n",
    "        self.db = np.zeros_like(self.biases)\n",
    "        self.v_dw = np.zeros_like(self.weights)\n",
    "        self.v_db = np.zeros_like(self.biases)\n",
    "        self.s_dw = np.zeros_like(self.weights)\n",
    "        self.s_db = np.zeros_like(self.biases)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, inputs, grad_output, learning_rate):\n",
    "        grad_inputs = np.dot(grad_output, self.weights.T)\n",
    "        grad_weights = np.dot(inputs.T, grad_output)\n",
    "        grad_biases = np.sum(grad_output, axis=0, keepdims=True)\n",
    "        \n",
    "        # Adam optimizer\n",
    "        beta1 = 0.9\n",
    "        beta2 = 0.999\n",
    "        epsilon = 1e-7\n",
    "\n",
    "        self.v_dw = beta1 * self.v_dw + (1 - beta1) * grad_weights\n",
    "        self.v_db = beta1 * self.v_db + (1 - beta1) * grad_biases\n",
    "        self.s_dw = beta2 * self.s_dw + (1 - beta2) * (grad_weights ** 2)\n",
    "        self.s_db = beta2 * self.s_db + (1 - beta2) * (grad_biases ** 2)\n",
    "        \n",
    "        v_dw_corrected = self.v_dw / (1 - beta1)\n",
    "        v_db_corrected = self.v_db / (1 - beta1)\n",
    "        s_dw_corrected = self.s_dw / (1 - beta2)\n",
    "        s_db_corrected = self.s_db / (1 - beta2)\n",
    "        \n",
    "        self.weights -= learning_rate * v_dw_corrected / (np.sqrt(s_dw_corrected) + epsilon)\n",
    "        self.biases -= learning_rate * v_db_corrected / (np.sqrt(s_db_corrected) + epsilon)\n",
    "\n",
    "class ReluActivation:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "class SoftmaxActivation:\n",
    "    def forward(self, inputs):\n",
    "        normalized_inputs = np.exp(inputs - np.amax(inputs, axis=1, keepdims=True))\n",
    "        self.output = normalized_inputs / np.sum(normalized_inputs, axis=1, keepdims=True)\n",
    "\n",
    "class CrossEntropyLoss:\n",
    "    @staticmethod\n",
    "    def calculate_loss(y_pred, y_true):\n",
    "        clipped_values = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = clipped_values[range(len(clipped_values)), y_true]\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(clipped_values * y_true, axis=1)\n",
    "        return np.mean(-np.log(correct_confidences))\n",
    "\n",
    "def calculate_accuracy(y_pred, y_true):\n",
    "    predicted_classes = np.argmax(y_pred, axis=1)\n",
    "    correct_predictions = np.sum(predicted_classes == y_true)\n",
    "    accuracy = correct_predictions / len(y_true)\n",
    "    return accuracy\n",
    "\n",
    "# Create dummy data with adjusted features\n",
    "X, y = create_data(100, 3)\n",
    "\n",
    "# Normalize data\n",
    "X = normalize(X)\n",
    "\n",
    "# Initialize layers and activations\n",
    "l1 = Layer(64, 128)  # Increased neurons in the first layer for better representation\n",
    "ac1 = ReluActivation()\n",
    "l2 = Layer(128, 64)  # Added an additional hidden layer\n",
    "ac2 = ReluActivation()\n",
    "l3 = Layer(64, 3)    # Output layer should match the number of classes (3)\n",
    "ac3 = SoftmaxActivation()\n",
    "\n",
    "# Training loop parameters\n",
    "epochs = 11000  # Reduced number of epochs to avoid overfitting\n",
    "learning_rate = 0.01  # Adjusted learning rate\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    l1.forward(X)\n",
    "    ac1.forward(l1.output)\n",
    "    l2.forward(ac1.output)\n",
    "    ac2.forward(l2.output)\n",
    "    l3.forward(ac2.output)\n",
    "    ac3.forward(l3.output)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = CrossEntropyLoss.calculate_loss(ac3.output, y)\n",
    "\n",
    "    # Backward pass (Gradient calculation)\n",
    "    grad_ac3 = ac3.output.copy()\n",
    "    grad_ac3[range(len(y)), y] -= 1\n",
    "    grad_ac3 /= len(y)\n",
    "\n",
    "    # Backpropagate gradients\n",
    "    ac3_backward = grad_ac3\n",
    "    l3.backward(ac2.output, ac3_backward, learning_rate)\n",
    "\n",
    "    ac2_backward = np.dot(ac3_backward, l3.weights.T)\n",
    "    l2.backward(ac1.output, ac2_backward, learning_rate)\n",
    "\n",
    "    ac1_backward = np.dot(ac2_backward, l2.weights.T)\n",
    "    l1.backward(X, ac1_backward, learning_rate)\n",
    "\n",
    "    # Print loss and accuracy every 500 epochs\n",
    "    if epoch % 500 == 0:\n",
    "        accuracy = calculate_accuracy(ac3.output, y)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Final loss and accuracy\n",
    "l1.forward(X)\n",
    "ac1.forward(l1.output)\n",
    "l2.forward(ac1.output)\n",
    "ac2.forward(l2.output)\n",
    "l3.forward(ac2.output)\n",
    "ac3.forward(l3.output)\n",
    "final_loss = CrossEntropyLoss.calculate_loss(ac3.output, y)\n",
    "final_accuracy = calculate_accuracy(ac3.output, y)\n",
    "print(np.argmax(ac3.output, axis=1), \"sss\", y)\n",
    "print(\"Final Loss:\", final_loss)\n",
    "print(\"Final Accuracy:\", final_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
