{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6bf4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.1094670054316003\n",
      "Epoch 100, Loss: 1.0745517483650424\n",
      "Epoch 200, Loss: 1.0737399278277269\n",
      "Epoch 300, Loss: 1.072957857797186\n",
      "Epoch 400, Loss: 1.07022647757383\n",
      "Epoch 500, Loss: 1.067985652857329\n",
      "Epoch 600, Loss: 1.066444726521256\n",
      "Epoch 700, Loss: 1.0653038545288402\n",
      "Epoch 800, Loss: 1.0635180115419423\n",
      "Epoch 900, Loss: 1.0617271866188989\n",
      "Epoch 1000, Loss: 1.0600922990285588\n",
      "Epoch 1100, Loss: 1.058547248483651\n",
      "Epoch 1200, Loss: 1.0571115013287646\n",
      "Epoch 1300, Loss: 1.0558425388814192\n",
      "Epoch 1400, Loss: 1.054617773153044\n",
      "Epoch 1500, Loss: 1.0535756702968293\n",
      "Epoch 1600, Loss: 1.0526693364190107\n",
      "Epoch 1700, Loss: 1.0516511572801914\n",
      "Epoch 1800, Loss: 1.0507272538145553\n",
      "Epoch 1900, Loss: 1.0499530320499788\n",
      "Epoch 2000, Loss: 1.0489732408632213\n",
      "Epoch 2100, Loss: 1.048221098874625\n",
      "Epoch 2200, Loss: 1.0474372056622203\n",
      "Epoch 2300, Loss: 1.0466452896246101\n",
      "Epoch 2400, Loss: 1.045933907081059\n",
      "Epoch 2500, Loss: 1.0454057862008945\n",
      "Epoch 2600, Loss: 1.0447714434415507\n",
      "Epoch 2700, Loss: 1.0440575126212759\n",
      "Epoch 2800, Loss: 1.043614204966714\n",
      "Epoch 2900, Loss: 1.0430944271648992\n",
      "Epoch 3000, Loss: 1.0426351980018043\n",
      "Epoch 3100, Loss: 1.0421097255359328\n",
      "Epoch 3200, Loss: 1.0415018319231095\n",
      "Epoch 3300, Loss: 1.041118557206512\n",
      "Epoch 3400, Loss: 1.0405308696140814\n",
      "Epoch 3500, Loss: 1.0402051038669529\n",
      "Epoch 3600, Loss: 1.0398725007931555\n",
      "Epoch 3700, Loss: 1.039453845549797\n",
      "Epoch 3800, Loss: 1.0394260051754152\n",
      "Epoch 3900, Loss: 1.0389274740098595\n",
      "Epoch 4000, Loss: 1.0386088221983278\n",
      "Epoch 4100, Loss: 1.038465199621433\n",
      "Epoch 4200, Loss: 1.0382820528035985\n",
      "Epoch 4300, Loss: 1.0379691057875595\n",
      "Epoch 4400, Loss: 1.0377202167408437\n",
      "Epoch 4500, Loss: 1.0373611755274204\n",
      "Epoch 4600, Loss: 1.0370002981164848\n",
      "Epoch 4700, Loss: 1.0365560483416896\n",
      "Epoch 4800, Loss: 1.0361340942262371\n",
      "Epoch 4900, Loss: 1.0356646261474145\n",
      "Epoch 5000, Loss: 1.0356857987160932\n",
      "Epoch 5100, Loss: 1.0352225937416752\n",
      "Epoch 5200, Loss: 1.034469626799276\n",
      "Epoch 5300, Loss: 1.0345177466870825\n",
      "Epoch 5400, Loss: 1.0337968126254529\n",
      "Epoch 5500, Loss: 1.0334933026833986\n",
      "Epoch 5600, Loss: 1.0333156588089525\n",
      "Epoch 5700, Loss: 1.0329184314450175\n",
      "Epoch 5800, Loss: 1.0329778460908834\n",
      "Epoch 5900, Loss: 1.0323978945829546\n",
      "Epoch 6000, Loss: 1.0321661288623496\n",
      "Epoch 6100, Loss: 1.0319270758421029\n",
      "Epoch 6200, Loss: 1.0317016854151995\n",
      "Epoch 6300, Loss: 1.0315211086247105\n",
      "Epoch 6400, Loss: 1.0312888401578968\n",
      "Epoch 6500, Loss: 1.0310794899581788\n",
      "Epoch 6600, Loss: 1.0316037113907035\n",
      "Epoch 6700, Loss: 1.0306801984156166\n",
      "Epoch 6800, Loss: 1.0312405643733598\n",
      "Epoch 6900, Loss: 1.0301878439221948\n",
      "Epoch 7000, Loss: 1.029902033753618\n",
      "Epoch 7100, Loss: 1.0296070194352938\n",
      "Epoch 7200, Loss: 1.029284006588318\n",
      "Epoch 7300, Loss: 1.0289820542384303\n",
      "Epoch 7400, Loss: 1.0286239317579244\n",
      "Epoch 7500, Loss: 1.028442085226653\n",
      "Epoch 7600, Loss: 1.0279622502664003\n",
      "Epoch 7700, Loss: 1.0275809565839487\n",
      "Epoch 7800, Loss: 1.0273108325059574\n",
      "Epoch 7900, Loss: 1.0269752768017146\n",
      "Epoch 8000, Loss: 1.0266034880302386\n",
      "Epoch 8100, Loss: 1.026403690235857\n",
      "Epoch 8200, Loss: 1.0261168604952744\n",
      "Epoch 8300, Loss: 1.0261159732345668\n",
      "Epoch 8400, Loss: 1.0255533444224303\n",
      "Epoch 8500, Loss: 1.0252617295168582\n",
      "Epoch 8600, Loss: 1.0250514411159384\n",
      "Epoch 8700, Loss: 1.0247156585503414\n",
      "Epoch 8800, Loss: 1.0244594564827851\n",
      "Epoch 8900, Loss: 1.024369777492465\n",
      "Epoch 9000, Loss: 1.0239123819916807\n",
      "Epoch 9100, Loss: 1.0235958165392898\n",
      "Epoch 9200, Loss: 1.0232531615673413\n",
      "Epoch 9300, Loss: 1.0230050510481394\n",
      "Epoch 9400, Loss: 1.022692656283175\n",
      "Epoch 9500, Loss: 1.0223619440799614\n",
      "Epoch 9600, Loss: 1.022078965817307\n",
      "Epoch 9700, Loss: 1.0218036743417902\n",
      "Epoch 9800, Loss: 1.0215169049288355\n",
      "Epoch 9900, Loss: 1.0212056820150281\n",
      "Epoch 10000, Loss: 1.020982678702251\n",
      "Epoch 10100, Loss: 1.0207055813549708\n",
      "Epoch 10200, Loss: 1.0204664174021862\n",
      "Epoch 10300, Loss: 1.0202769984347204\n",
      "Epoch 10400, Loss: 1.0201012443874042\n",
      "Epoch 10500, Loss: 1.0198940293293879\n",
      "Epoch 10600, Loss: 1.0199028374402521\n",
      "Epoch 10700, Loss: 1.0194202664526417\n",
      "Epoch 10800, Loss: 1.0190204650348695\n",
      "Epoch 10900, Loss: 1.018757100862632\n",
      "Epoch 11000, Loss: 1.018527514834916\n",
      "Epoch 11100, Loss: 1.0182814585855304\n",
      "Epoch 11200, Loss: 1.0180394627268203\n",
      "Epoch 11300, Loss: 1.017799340069275\n",
      "Epoch 11400, Loss: 1.0176287718998889\n",
      "Epoch 11500, Loss: 1.0178865791396503\n",
      "Epoch 11600, Loss: 1.017155626994314\n",
      "Epoch 11700, Loss: 1.0170720580253407\n",
      "Epoch 11800, Loss: 1.0169196848675162\n",
      "Epoch 11900, Loss: 1.0167665754598079\n",
      "Epoch 12000, Loss: 1.0166264635747593\n",
      "Epoch 12100, Loss: 1.016389236026159\n",
      "Epoch 12200, Loss: 1.0161905148728576\n",
      "Epoch 12300, Loss: 1.0159885667200117\n",
      "Epoch 12400, Loss: 1.0162948797553184\n",
      "Epoch 12500, Loss: 1.015562064764186\n",
      "Epoch 12600, Loss: 1.0153241375194844\n",
      "Epoch 12700, Loss: 1.0150969151710394\n",
      "Epoch 12800, Loss: 1.0152254979245703\n",
      "Epoch 12900, Loss: 1.014664858579997\n",
      "Epoch 13000, Loss: 1.0144955615431996\n",
      "Epoch 13100, Loss: 1.014344274357353\n",
      "Epoch 13200, Loss: 1.0141258958181356\n",
      "Epoch 13300, Loss: 1.0139503436792388\n",
      "Epoch 13400, Loss: 1.013772890877469\n",
      "Epoch 13500, Loss: 1.0135738473226708\n",
      "Epoch 13600, Loss: 1.0134120257635169\n",
      "Epoch 13700, Loss: 1.013236388997356\n",
      "Epoch 13800, Loss: 1.0131319529865186\n",
      "Epoch 13900, Loss: 1.012873961983061\n",
      "Epoch 14000, Loss: 1.012738128267357\n",
      "Epoch 14100, Loss: 1.01245285353364\n",
      "Epoch 14200, Loss: 1.0126877629134845\n",
      "Epoch 14300, Loss: 1.0123020896545023\n",
      "Epoch 14400, Loss: 1.011856641249279\n",
      "Epoch 14500, Loss: 1.0115606353499764\n",
      "Epoch 14600, Loss: 1.0114378384451106\n",
      "Epoch 14700, Loss: 1.0110982989309036\n",
      "Epoch 14800, Loss: 1.010913781436737\n",
      "Epoch 14900, Loss: 1.0106420349343779\n",
      "Epoch 15000, Loss: 1.0103824798262866\n",
      "Epoch 15100, Loss: 1.0102140294698236\n",
      "Epoch 15200, Loss: 1.009947757596228\n",
      "Epoch 15300, Loss: 1.0097743119095643\n",
      "Epoch 15400, Loss: 1.0096336965475747\n",
      "Epoch 15500, Loss: 1.009439708446499\n",
      "Epoch 15600, Loss: 1.0092762023728548\n",
      "Epoch 15700, Loss: 1.0090983939158624\n",
      "Epoch 15800, Loss: 1.0089311015506102\n",
      "Epoch 15900, Loss: 1.0087369754027973\n",
      "Epoch 16000, Loss: 1.008576191519301\n",
      "Epoch 16100, Loss: 1.0084167445529233\n",
      "Epoch 16200, Loss: 1.0083232547056653\n",
      "Epoch 16300, Loss: 1.0082039329131844\n",
      "Epoch 16400, Loss: 1.0080725811460873\n",
      "Epoch 16500, Loss: 1.0080610139036583\n",
      "Epoch 16600, Loss: 1.0082783809473226\n",
      "Epoch 16700, Loss: 1.0079285809868752\n",
      "Epoch 16800, Loss: 1.0078582041223612\n",
      "Epoch 16900, Loss: 1.0077702999895624\n",
      "Epoch 17000, Loss: 1.0077131312246719\n",
      "Epoch 17100, Loss: 1.007872831743427\n",
      "Epoch 17200, Loss: 1.0075666906217755\n",
      "Epoch 17300, Loss: 1.0092185505770916\n",
      "Epoch 17400, Loss: 1.0074237943219888\n",
      "Epoch 17500, Loss: 1.0073630137553231\n",
      "Epoch 17600, Loss: 1.0073087803907887\n",
      "Epoch 17700, Loss: 1.0072355464570588\n",
      "Epoch 17800, Loss: 1.0071953774079239\n",
      "Epoch 17900, Loss: 1.0080280815564806\n",
      "Epoch 18000, Loss: 1.0070327389157163\n",
      "Epoch 18100, Loss: 1.0069009478572704\n",
      "Epoch 18200, Loss: 1.006916797851578\n",
      "Epoch 18300, Loss: 1.0068320129914325\n",
      "Epoch 18400, Loss: 1.0067292349318453\n",
      "Epoch 18500, Loss: 1.0068303697317038\n",
      "Epoch 18600, Loss: 1.0072231019559263\n",
      "Epoch 18700, Loss: 1.006603371469327\n",
      "Epoch 18800, Loss: 1.0065603534585779\n",
      "Epoch 18900, Loss: 1.0064283507120888\n",
      "Epoch 19000, Loss: 1.0064605641443525\n",
      "Epoch 19100, Loss: 1.00687053309653\n",
      "Epoch 19200, Loss: 1.0066859321226496\n",
      "Epoch 19300, Loss: 1.006351500765952\n",
      "Epoch 19400, Loss: 1.0062229602815522\n",
      "Epoch 19500, Loss: 1.0062761545894883\n",
      "Epoch 19600, Loss: 1.0061207098722316\n",
      "Epoch 19700, Loss: 1.0060924805927385\n",
      "Epoch 19800, Loss: 1.0060300286749642\n",
      "Epoch 19900, Loss: 1.0060881385720786\n",
      "Epoch 20000, Loss: 1.006148237602427\n",
      "Epoch 20100, Loss: 1.0059372888724303\n",
      "Epoch 20200, Loss: 1.005892854183339\n",
      "Epoch 20300, Loss: 1.0057175906471378\n",
      "Epoch 20400, Loss: 1.0057303361923138\n",
      "Epoch 20500, Loss: 1.005691548684521\n",
      "Epoch 20600, Loss: 1.0057987397629309\n",
      "Epoch 20700, Loss: 1.005792433635247\n",
      "Epoch 20800, Loss: 1.0055564015430605\n",
      "Epoch 20900, Loss: 1.0054982752728006\n",
      "Epoch 21000, Loss: 1.005442528670334\n",
      "Epoch 21100, Loss: 1.0053861285798427\n",
      "Epoch 21200, Loss: 1.005363782551092\n",
      "Epoch 21300, Loss: 1.005236474765502\n",
      "Epoch 21400, Loss: 1.0052545820079606\n",
      "Epoch 21500, Loss: 1.0052350085680677\n",
      "Epoch 21600, Loss: 1.0052437865227701\n",
      "Epoch 21700, Loss: 1.00510325352296\n",
      "Epoch 21800, Loss: 1.005050595112917\n",
      "Epoch 21900, Loss: 1.0050065172987266\n",
      "Epoch 22000, Loss: 1.0053712512627007\n",
      "Epoch 22100, Loss: 1.0048684689625507\n",
      "Epoch 22200, Loss: 1.0048049655627709\n",
      "Epoch 22300, Loss: 1.0048022859478982\n",
      "Epoch 22400, Loss: 1.0049906408558587\n",
      "Epoch 22500, Loss: 1.0048202957450172\n",
      "Epoch 22600, Loss: 1.004561322064076\n",
      "Epoch 22700, Loss: 1.0045056416098028\n",
      "Epoch 22800, Loss: 1.0044980618077963\n",
      "Epoch 22900, Loss: 1.0042552921555057\n",
      "Epoch 23000, Loss: 1.0042938393955432\n",
      "Epoch 23100, Loss: 1.0042438465025216\n",
      "Epoch 23200, Loss: 1.0042079982969063\n",
      "Epoch 23300, Loss: 1.004043181671575\n",
      "Epoch 23400, Loss: 1.0043854797019436\n",
      "Epoch 23500, Loss: 1.0039482809763027\n",
      "Epoch 23600, Loss: 1.0039079940224709\n",
      "Epoch 23700, Loss: 1.0043139587239436\n",
      "Epoch 23800, Loss: 1.0037686922817826\n",
      "Epoch 23900, Loss: 1.0036904412132703\n",
      "Epoch 24000, Loss: 1.0036469517638371\n",
      "Epoch 24100, Loss: 1.0040343629211945\n",
      "Epoch 24200, Loss: 1.0038097221546576\n",
      "Epoch 24300, Loss: 1.0034680197994237\n",
      "Epoch 24400, Loss: 1.003610556637378\n",
      "Epoch 24500, Loss: 1.0032581706650199\n",
      "Epoch 24600, Loss: 1.0033086580879493\n",
      "Epoch 24700, Loss: 1.0032349402251006\n",
      "Epoch 24800, Loss: 1.0031947288017882\n",
      "Epoch 24900, Loss: 1.0030335760061522\n",
      "Epoch 25000, Loss: 1.0031770657946024\n",
      "Epoch 25100, Loss: 1.0030127632077313\n",
      "Epoch 25200, Loss: 1.0029817963914311\n",
      "Epoch 25300, Loss: 1.0028949398811222\n",
      "Epoch 25400, Loss: 1.0027769803262672\n",
      "Epoch 25500, Loss: 1.002758357669749\n",
      "Epoch 25600, Loss: 1.0027199128939273\n",
      "Epoch 25700, Loss: 1.0026713355294883\n",
      "Epoch 25800, Loss: 1.0024910163710392\n",
      "Epoch 25900, Loss: 1.0025652025754694\n",
      "Epoch 26000, Loss: 1.00246017328695\n",
      "Epoch 26100, Loss: 1.0026480049886721\n",
      "Epoch 26200, Loss: 1.0024064836153515\n",
      "Epoch 26300, Loss: 1.0024541525882773\n",
      "Epoch 26400, Loss: 1.002293227617336\n",
      "Epoch 26500, Loss: 1.0023142221243795\n",
      "Epoch 26600, Loss: 1.0022217280495835\n",
      "Epoch 26700, Loss: 1.002030860995564\n",
      "Epoch 26800, Loss: 1.0021342538383353\n",
      "Epoch 26900, Loss: 1.0020873318886834\n",
      "Epoch 27000, Loss: 1.0021553321746621\n",
      "Epoch 27100, Loss: 1.0018995572787015\n",
      "Epoch 27200, Loss: 1.001908216675279\n",
      "Epoch 27300, Loss: 1.0021051683656583\n",
      "Epoch 27400, Loss: 1.001972819962055\n",
      "Epoch 27500, Loss: 1.001769234387047\n",
      "Epoch 27600, Loss: 1.001816559426028\n",
      "Epoch 27700, Loss: 1.0017419564536134\n",
      "Epoch 27800, Loss: 1.0016747993969317\n",
      "Epoch 27900, Loss: 1.0018561212259411\n",
      "Epoch 28000, Loss: 1.00293899439912\n",
      "Epoch 28100, Loss: 1.0015568669674852\n",
      "Epoch 28200, Loss: 1.0017027384921788\n",
      "Epoch 28300, Loss: 1.0014301625222402\n",
      "Epoch 28400, Loss: 1.001559787184035\n",
      "Epoch 28500, Loss: 1.0014147079987357\n",
      "Epoch 28600, Loss: 1.0015711266145149\n",
      "Epoch 28700, Loss: 1.0012746912188035\n",
      "Epoch 28800, Loss: 1.0011918873745813\n",
      "Epoch 28900, Loss: 1.0011931461832484\n",
      "Epoch 29000, Loss: 1.001235656960016\n",
      "Epoch 29100, Loss: 1.0010833652560234\n",
      "Epoch 29200, Loss: 1.001170540247579\n",
      "Epoch 29300, Loss: 1.0010286316194223\n",
      "Epoch 29400, Loss: 1.0010945624529226\n",
      "Epoch 29500, Loss: 1.00105873672635\n",
      "Epoch 29600, Loss: 1.0010548258918133\n",
      "Epoch 29700, Loss: 1.0009962081463262\n",
      "Epoch 29800, Loss: 1.0010487250123181\n",
      "Epoch 29900, Loss: 1.0009076401354642\n",
      "Epoch 30000, Loss: 1.000865749195807\n",
      "Epoch 30100, Loss: 1.0014480780336974\n",
      "Epoch 30200, Loss: 1.0008014292201408\n",
      "Epoch 30300, Loss: 1.0008144945461959\n",
      "Epoch 30400, Loss: 1.0013297272944064\n",
      "Epoch 30500, Loss: 1.0012910320540298\n",
      "Epoch 30600, Loss: 1.0008688451857484\n",
      "Epoch 30700, Loss: 1.000748349686584\n",
      "Epoch 30800, Loss: 1.0007230018138222\n",
      "Epoch 30900, Loss: 1.0007982085762066\n",
      "Epoch 31000, Loss: 1.0006393345754436\n",
      "Epoch 31100, Loss: 1.0006481274131953\n",
      "Epoch 31200, Loss: 1.0005696740325492\n",
      "Epoch 31300, Loss: 1.0005384323241977\n",
      "Epoch 31400, Loss: 1.0008047694096052\n",
      "Epoch 31500, Loss: 1.0006752281176157\n",
      "Epoch 31600, Loss: 1.0004531198345203\n",
      "Epoch 31700, Loss: 1.0004667669824938\n",
      "Epoch 31800, Loss: 1.000416616674336\n",
      "Epoch 31900, Loss: 1.0003889841238995\n",
      "Epoch 32000, Loss: 1.00041094864349\n",
      "Epoch 32100, Loss: 1.000309102053874\n",
      "Epoch 32200, Loss: 1.0003206241520926\n",
      "Epoch 32300, Loss: 1.00051246582866\n",
      "Epoch 32400, Loss: 1.000285364896616\n",
      "Epoch 32500, Loss: 1.0002448567390172\n",
      "Epoch 32600, Loss: 1.0002478842704268\n",
      "Epoch 32700, Loss: 1.0002055583499012\n",
      "Epoch 32800, Loss: 1.0011205713963085\n",
      "Epoch 32900, Loss: 1.000197522010724\n",
      "Epoch 33000, Loss: 1.0002068721924275\n",
      "Epoch 33100, Loss: 1.0001334560612058\n",
      "Epoch 33200, Loss: 1.0001475499697785\n",
      "Epoch 33300, Loss: 1.0001460457156597\n",
      "Epoch 33400, Loss: 1.0000367944519681\n",
      "Epoch 33500, Loss: 1.0001080637764475\n",
      "Epoch 33600, Loss: 1.0001050514370753\n",
      "Epoch 33700, Loss: 1.0005926087705193\n",
      "Epoch 33800, Loss: 1.000077941423202\n",
      "Epoch 33900, Loss: 1.000288066615369\n",
      "Epoch 34000, Loss: 1.0000544943311236\n",
      "Epoch 34100, Loss: 1.0000500523346063\n",
      "Epoch 34200, Loss: 1.0001197983729428\n",
      "Epoch 34300, Loss: 1.0000417830419357\n",
      "Epoch 34400, Loss: 1.0000539332033869\n",
      "Epoch 34500, Loss: 1.0000020208090126\n",
      "Epoch 34600, Loss: 0.9999826728566942\n",
      "Epoch 34700, Loss: 0.99997960181897\n",
      "Epoch 34800, Loss: 1.0002323916568636\n",
      "Epoch 34900, Loss: 0.9999616458222635\n",
      "Epoch 35000, Loss: 1.0000175180078585\n",
      "Epoch 35100, Loss: 0.9999454111657856\n",
      "Epoch 35200, Loss: 0.9999408609030578\n",
      "Epoch 35300, Loss: 0.9999177561915212\n",
      "Epoch 35400, Loss: 0.9999337710936049\n",
      "Epoch 35500, Loss: 1.000279234740964\n",
      "Epoch 35600, Loss: 0.9999271386572313\n",
      "Epoch 35700, Loss: 0.9999211294628568\n",
      "Epoch 35800, Loss: 1.000505624659307\n",
      "Epoch 35900, Loss: 0.9999160861673236\n",
      "Epoch 36000, Loss: 0.9999148496630506\n",
      "Epoch 36100, Loss: 0.9998682831860644\n",
      "Epoch 36200, Loss: 0.9998867978634606\n",
      "Epoch 36300, Loss: 0.9999084642896439\n",
      "Epoch 36400, Loss: 1.0004292271982662\n",
      "Epoch 36500, Loss: 0.9998927625693538\n",
      "Epoch 36600, Loss: 0.9998997555880703\n",
      "Epoch 36700, Loss: 0.9999091062229392\n",
      "Epoch 36800, Loss: 0.9998942961194989\n",
      "Epoch 36900, Loss: 0.9999269344002185\n",
      "Epoch 37000, Loss: 0.9998899504073219\n",
      "Epoch 37100, Loss: 0.9998880048171526\n",
      "Epoch 37200, Loss: 0.9998880161721649\n",
      "Epoch 37300, Loss: 0.9998893944968322\n",
      "Epoch 37400, Loss: 0.9998888763317639\n",
      "Epoch 37500, Loss: 0.9998945844961997\n",
      "Epoch 37600, Loss: 1.000072275349224\n",
      "Epoch 37700, Loss: 0.9999638079904309\n",
      "Epoch 37800, Loss: 0.9999011166303643\n",
      "Epoch 37900, Loss: 0.9999053085191766\n",
      "Epoch 38000, Loss: 0.9999097161659267\n",
      "Epoch 38100, Loss: 0.9999158849526028\n",
      "Epoch 38200, Loss: 1.0002167965565147\n",
      "Epoch 38300, Loss: 0.999926452907553\n",
      "Epoch 38400, Loss: 1.0005854260682858\n",
      "Epoch 38500, Loss: 1.0000095702985503\n",
      "Epoch 38600, Loss: 0.999944306867665\n",
      "Epoch 38700, Loss: 0.9999552025408658\n",
      "Epoch 38800, Loss: 1.0000529410739463\n",
      "Epoch 38900, Loss: 0.9999563054018953\n",
      "Epoch 39000, Loss: 1.000001632874195\n",
      "Epoch 39100, Loss: 1.000557801850003\n",
      "Epoch 39200, Loss: 1.0000117919694282\n",
      "Epoch 39300, Loss: 1.000023582543362\n",
      "Epoch 39400, Loss: 1.0000367407465376\n",
      "Epoch 39500, Loss: 1.0000143433091921\n",
      "Epoch 39600, Loss: 1.0003510849875898\n",
      "Epoch 39700, Loss: 1.0000349041435619\n",
      "Epoch 39800, Loss: 1.0000895956751203\n",
      "Epoch 39900, Loss: 1.0001015586934496\n",
      "Epoch 40000, Loss: 1.000115489969597\n",
      "Epoch 40100, Loss: 1.0001292260754302\n",
      "Epoch 40200, Loss: 1.0044547106807662\n",
      "Epoch 40300, Loss: 1.000172323482523\n",
      "Epoch 40400, Loss: 1.0001665521458425\n",
      "Epoch 40500, Loss: 1.001627237150196\n",
      "Epoch 40600, Loss: 1.000198010324375\n",
      "Epoch 40700, Loss: 1.0002122583978368\n",
      "Epoch 40800, Loss: 1.0002336217533954\n",
      "Epoch 40900, Loss: 1.0002553173330013\n",
      "Epoch 41000, Loss: 1.0011615670064808\n",
      "Epoch 41100, Loss: 1.0002827554380471\n",
      "Epoch 41200, Loss: 1.0003200802789434\n",
      "Epoch 41300, Loss: 1.0003525517243812\n",
      "Epoch 41400, Loss: 1.0012177577837058\n",
      "Epoch 41500, Loss: 1.0003933866735295\n",
      "Epoch 41600, Loss: 1.0004578492522198\n",
      "Epoch 41700, Loss: 1.0004609951796741\n",
      "Epoch 41800, Loss: 1.000648599930001\n",
      "Epoch 41900, Loss: 1.001012948190144\n",
      "Epoch 42000, Loss: 1.0005369480749764\n",
      "Epoch 42100, Loss: 1.0005832230286842\n",
      "Epoch 42200, Loss: 1.0006317726853027\n",
      "Epoch 42300, Loss: 1.0006710215113874\n",
      "Epoch 42400, Loss: 1.0007106967440447\n",
      "Epoch 42500, Loss: 1.002523064599135\n",
      "Epoch 42600, Loss: 1.0007847709179596\n",
      "Epoch 42700, Loss: 1.0008101777255805\n",
      "Epoch 42800, Loss: 1.0008207178154045\n",
      "Epoch 42900, Loss: 1.0008529701485755\n",
      "Epoch 43000, Loss: 1.0008913625435862\n",
      "Epoch 43100, Loss: 1.0013126724489363\n",
      "Epoch 43200, Loss: 1.0009580919823504\n",
      "Epoch 43300, Loss: 1.0010795209693173\n",
      "Epoch 43400, Loss: 1.0010483519975517\n",
      "Epoch 43500, Loss: 1.0017316088533281\n",
      "Epoch 43600, Loss: 1.001137790064576\n",
      "Epoch 43700, Loss: 1.0011451912860998\n",
      "Epoch 43800, Loss: 1.0011828414818287\n",
      "Epoch 43900, Loss: 1.00122156771532\n",
      "Epoch 44000, Loss: 1.0012615351707483\n",
      "Epoch 44100, Loss: 1.0012920935336536\n",
      "Epoch 44200, Loss: 1.0013397288957708\n",
      "Epoch 44300, Loss: 1.0022028546811956\n",
      "Epoch 44400, Loss: 1.0016162682902832\n",
      "Epoch 44500, Loss: 1.001464844108857\n",
      "Epoch 44600, Loss: 1.001520007174026\n",
      "Epoch 44700, Loss: 1.001623260709661\n",
      "Epoch 44800, Loss: 1.0015047969392394\n",
      "Epoch 44900, Loss: 1.0021935231267887\n",
      "Epoch 45000, Loss: 1.0017128137248907\n",
      "Epoch 45100, Loss: 1.0017053243057688\n",
      "Epoch 45200, Loss: 1.0017502947964703\n",
      "Epoch 45300, Loss: 1.0017446510195809\n",
      "Epoch 45400, Loss: 1.0027472847751584\n",
      "Epoch 45500, Loss: 1.0023551787013745\n",
      "Epoch 45600, Loss: 1.0018840141088357\n",
      "Epoch 45700, Loss: 1.0019076544482914\n",
      "Epoch 45800, Loss: 1.0019443320939263\n",
      "Epoch 45900, Loss: 1.0019842689701846\n",
      "Epoch 46000, Loss: 1.0031144623318555\n",
      "Epoch 46100, Loss: 1.002078306430229\n",
      "Epoch 46200, Loss: 1.0021000208085975\n",
      "Epoch 46300, Loss: 1.002858686492483\n",
      "Epoch 46400, Loss: 1.0021613383014838\n",
      "Epoch 46500, Loss: 1.0022197856201531\n",
      "Epoch 46600, Loss: 1.0022594175025965\n",
      "Epoch 46700, Loss: 1.0022927251243494\n",
      "Epoch 46800, Loss: 1.002324136199446\n",
      "Epoch 46900, Loss: 1.0023349492715468\n",
      "Epoch 47000, Loss: 1.0023586198080643\n",
      "Epoch 47100, Loss: 1.0024080824468755\n",
      "Epoch 47200, Loss: 1.0023704840085501\n",
      "Epoch 47300, Loss: 1.0023659793924764\n",
      "Epoch 47400, Loss: 1.0024766437274943\n",
      "Epoch 47500, Loss: 1.0023630219037771\n",
      "Epoch 47600, Loss: 1.0023520508075103\n",
      "Epoch 47700, Loss: 1.0023846807297547\n",
      "Epoch 47800, Loss: 1.0026043792672144\n",
      "Epoch 47900, Loss: 1.0023239595983375\n",
      "Epoch 48000, Loss: 1.0023159835589484\n",
      "Epoch 48100, Loss: 1.002253821080443\n",
      "Epoch 48200, Loss: 1.0022711403138858\n",
      "Epoch 48300, Loss: 1.0022858540649124\n",
      "Epoch 48400, Loss: 1.002286216596794\n",
      "Epoch 48500, Loss: 1.0024242842680806\n",
      "Epoch 48600, Loss: 1.0022681866359433\n",
      "Epoch 48700, Loss: 1.0022083840299658\n",
      "Epoch 48800, Loss: 1.00219188532681\n",
      "Epoch 48900, Loss: 1.002109945396331\n",
      "Epoch 49000, Loss: 1.002060229096527\n",
      "Epoch 49100, Loss: 1.0020510683365411\n",
      "Epoch 49200, Loss: 1.0019313804158627\n",
      "Epoch 49300, Loss: 1.0018605468046833\n",
      "Epoch 49400, Loss: 1.0025974576139929\n",
      "Epoch 49500, Loss: 1.00204601897603\n",
      "Epoch 49600, Loss: 1.001634857586022\n",
      "Epoch 49700, Loss: 1.0015980022926794\n",
      "Epoch 49800, Loss: 1.001732220920755\n",
      "Epoch 49900, Loss: 1.0015326409552672\n",
      "acc 0.4766666666666667\n",
      "z 570\n",
      "570 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2]\n",
      "Final Loss: 1.0014809339013868\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Function to create dummy data\n",
    "def create_data(points, classes): \n",
    "    x = np.zeros((points * classes, 64))  # Adjusted to 64 features\n",
    "    y = np.zeros(points * classes, dtype='uint8')\n",
    "    for class_number in range(classes):\n",
    "        ix = range(points * class_number, points * (class_number + 1))\n",
    "        r = np.linspace(0.0, 1, points)  # radius \n",
    "        t = np.linspace(class_number * 4, (class_number + 1) * 4, points) + np.random.randn(points) * 0.2\n",
    "        features = np.c_[r * np.sin(t * 2.5), r * np.cos(t * 2.5)]  # Generate 2 features\n",
    "        x[ix, :2] = features  # Place the 2 features in the first 2 columns\n",
    "        y[ix] = class_number\n",
    "    return x, y\n",
    "\n",
    "# Layers and activations\n",
    "class Layer:\n",
    "    def __init__(self, input_size, neuron_size):\n",
    "        self.weights = np.random.randn(input_size, neuron_size) * np.sqrt(2.0 / input_size)\n",
    "        self.biases = np.zeros((1, neuron_size))\n",
    "        self.dw = np.zeros_like(self.weights)\n",
    "        self.db = np.zeros_like(self.biases)\n",
    "        self.v_dw = np.zeros_like(self.weights)\n",
    "        self.v_db = np.zeros_like(self.biases)\n",
    "        self.s_dw = np.zeros_like(self.weights)\n",
    "        self.s_db = np.zeros_like(self.biases)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, inputs, grad_output, learning_rate):\n",
    "        grad_inputs = np.dot(grad_output, self.weights.T)\n",
    "        grad_weights = np.dot(inputs.T, grad_output)\n",
    "        grad_biases = np.sum(grad_output, axis=0, keepdims=True)\n",
    "        \n",
    "        # Adam optimizer\n",
    "        beta1 = 0.9\n",
    "        beta2 = 0.999\n",
    "        epsilon = 1e-7\n",
    "\n",
    "        self.v_dw = beta1 * self.v_dw + (1 - beta1) * grad_weights\n",
    "        self.v_db = beta1 * self.v_db + (1 - beta1) * grad_biases\n",
    "        self.s_dw = beta2 * self.s_dw + (1 - beta2) * (grad_weights ** 2)\n",
    "        self.s_db = beta2 * self.s_db + (1 - beta2) * (grad_biases ** 2)\n",
    "        \n",
    "        v_dw_corrected = self.v_dw / (1 - beta1)\n",
    "        v_db_corrected = self.v_db / (1 - beta1)\n",
    "        s_dw_corrected = self.s_dw / (1 - beta2)\n",
    "        s_db_corrected = self.s_db / (1 - beta2)\n",
    "        \n",
    "        self.weights -= learning_rate * v_dw_corrected / (np.sqrt(s_dw_corrected) + epsilon)\n",
    "        self.biases -= learning_rate * v_db_corrected / (np.sqrt(s_db_corrected) + epsilon)\n",
    "\n",
    "class ReluActivation:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "class SoftmaxActivation:\n",
    "    def forward(self, inputs):\n",
    "        normalized_inputs = np.exp(inputs - np.amax(inputs, axis=1, keepdims=True))\n",
    "        self.output = normalized_inputs / np.sum(normalized_inputs, axis=1, keepdims=True)\n",
    "\n",
    "class CrossEntropyLoss:\n",
    "    @staticmethod\n",
    "    def calculate_loss(y_pred, y_true):\n",
    "        clipped_values = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = clipped_values[range(len(clipped_values)), y_true]\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(clipped_values * y_true, axis=1)\n",
    "        return np.mean(-np.log(correct_confidences))\n",
    "\n",
    "# Create dummy data with adjusted features\n",
    "X, y = create_data(100, 3)\n",
    "\n",
    "# Initialize layers and activations\n",
    "l1 = Layer(64, 32)  # Adjusted input_size to match features in X\n",
    "ac1 = ReluActivation()\n",
    "l2 = Layer(32, 3)  # Output layer should match the number of classes (3)\n",
    "ac2 = SoftmaxActivation()\n",
    "\n",
    "# Training loop parameters\n",
    "epochs = 50000\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    l1.forward(X)\n",
    "    ac1.forward(l1.output)\n",
    "    l2.forward(ac1.output)\n",
    "    ac2.forward(l2.output)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = CrossEntropyLoss.calculate_loss(ac2.output, y)\n",
    "\n",
    "    # Backward pass (Gradient calculation)\n",
    "    grad_ac2 = ac2.output.copy()\n",
    "    grad_ac2[range(len(y)), y] -= 1\n",
    "    grad_ac2 /= len(y)\n",
    "\n",
    "    # Backpropagate gradients\n",
    "    ac2_backward = grad_ac2\n",
    "    l2.backward(ac1.output, ac2_backward, learning_rate)\n",
    "\n",
    "    ac1_backward = np.dot(ac2_backward, l2.weights.T)\n",
    "    l1.backward(X, ac1_backward, learning_rate)\n",
    "\n",
    "    # Print loss every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Final loss\n",
    "l1.forward(X)\n",
    "ac1.forward(l1.output)\n",
    "l2.forward(ac1.output)\n",
    "ac2.forward(l2.output)\n",
    "final_loss = CrossEntropyLoss.calculate_loss(ac2.output, y)\n",
    "\n",
    "print(\"acc\", np.mean(np.argmax(ac2.output, axis=1) == y))\n",
    "print(\"z\", np.argmax(ac2.output))\n",
    "print(np.argmax(ac2.output), y)\n",
    "print(\"Final Loss:\", final_loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
